
# Attention.Ai - Academic Research Paper Assistant üéì
This project is a fast, intuitive tool for researchers and students to search for academic papers, get quick summaries, and receive targeted answers to their questions!

Since the model requires approximately 9GB RAM, I highly suggest to use google colab notebook with T4 GPU Support, so that model can download, load and run without much complications.
The model may fail to load or run if proper support is not present in the local system.


```bash
    https://colab.research.google.com/drive/1BSeaZgDi3GkYNVD0t4_vLPF8fgN0vZEj?usp=sharing
```
To run streamlit on colab, steps are given below in Streamlit app section.
## Installation‚¨áÔ∏è
Neo4js for graph database
```bash
  !pip install Neo4js
  from neo4j import GraphDatabase
```
Transformers
```bash
  !pip install transformers
```

HuggingFaceü§ó

Login into your huggingface profile

You may need to accept the t&c to use Ministral 8B Instruct Model.
After that generate token from your profile.

```bash
  !huggingface-cli login
```
The model will first download itself which will take some time :) 
After that whenever model is called, it will just load its Technical "Shards Checkpoint".

Outlines
```bash
  !pip install outlines
```

The model, Ministral 8B is
Trained with a 128k context window with interleaved sliding-window attention
Trained on a large proportion of multilingual and code data
(So, better to run on Google Colab)



## DatabaseGeneration
The project utilizes the ArXiv API to retrieve data from millions of research papers based on the user‚Äôs prompt topic. This data is then structured into a Neo4j graph database, enabling fast, flexible retrieval. With nodes organized by key attributes like author, title, publication date, and more, the graph database allows users to easily explore relationships between papers and find relevant research insights effortlessly.




## Summarizer
In this setup, a quantized Ministral model generates responses based on user-selected papers,i.e User is asked the topic, After which all relevant papers are displayed by using a drop down key. The user can select a specific title to get the detailed summary along with the Github Repo link also (if available) and the actual paper link can also be accessed by clicking on 'Read More' which are processed by calling our LLM model.
## QnA
When a general question is requested, the system creates a prompt based on content, title along with the question by user for each paper, and the model generates an answer for this prompt. The answer is stored in a vector along with confidence scores.The user is presented with the answer based on highest-confidence score along with its source, plus an alternative option for comparison. This approach ensures users receive the most relevant information with confidence-based accuracy.
# Streamlit app
To run Streamlit app on colab, 
ensure these dependencies are installed or install them using

```bash
  !pip install streamlit pyngrok neo4j sentence-transformers
```
# Authenticate ngrok with your authtoken
This token is to be generated by logging in into https://dashboard.ngrok.com/ .

use this command to Authenticate in your cell
```bash
  !ngrok authtoken <YOUR TOKEN>
```

run the streamlit_app cell

once loaded, you can visit the ngrok host link to use the app.

To use on local system
run
```bash
  streamlit run app.py
```


## Demo Link
```bash
    https://drive.google.com/file/d/1JNILUmzOxwUTjozszy9jVrQTADhTFx7N/view?usp=drive_link
```
